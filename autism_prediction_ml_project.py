# -*- coding: utf-8 -*-
"""Autism Prediction ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_SiGRIw_s6A2cB-HxjYiDwkgpLxCCLY

# 1. Importing the dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV
from sklearn.linear_model import LogisticRegression , RidgeClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pickle

"""# Data Loading & Understanding"""

df = pd.read_csv("Autism.csv")
test_df=pd.read_csv("Autism_test.csv")
df.head()

df.shape

pd.set_option('display.max_columns', None)

df.info()

test_df.info()

# convert age column datatype to integer
df["age"] = df["age"].astype(int)
test_df["age"] = test_df["age"].astype(int)
df.head(2)

for col in df.columns:
  numerical_features = ["ID", "age", "result"]
  if col not in numerical_features:
    print(col, df[col].unique())
    print("-"*50)

# dropping ID & age_desc column
df = df.drop(columns=["ID", "age_desc"])
test_df=test_df.drop(columns=["age_desc"])

df.columns

df["contry_of_res"].unique()

# taget class distribution
df["Class/ASD"].value_counts()

"""# Insights:

1. missing values in ethnicity & relation

2. age_desc column has only 1 unique value. so it is removed as it is not important for prediction

3. fixed country names
4. identified class imbalance in the target column

# Exploratory Data Analysis (EDA)
"""

df.describe()

"""#Univariate Analysis

Numerical Columns:age , result
"""

# set the desired theme
sns.set_theme(style="darkgrid")

# Histogram for "age"

sns.histplot(df["age"], kde=True)
plt.title("Distribution of Age")

# calculate mean and median
age_mean = df["age"].mean()
age_median = df["age"].median()

print("Mean:", age_mean)
print("Median:", age_median)


# add vertical lines for mean and median
plt.axvline(age_mean, color="red", linestyle="--", label="Mean")
plt.axvline(age_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "age"-->TEST

sns.histplot(test_df["age"], kde=True)
plt.title("Distribution of Age")

# calculate mean and median
age_mean = test_df["age"].mean()
age_median = test_df["age"].median()

print("Mean:", age_mean)
print("Median:", age_median)


# add vertical lines for mean and median
plt.axvline(age_mean, color="red", linestyle="--", label="Mean")
plt.axvline(age_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "result"

sns.histplot(df["result"], kde=True)
plt.title("Distribution of result")

# calculate mean and median
result_mean = df["result"].mean()
result_median = df["result"].median()

print("Mean:", result_mean)
print("Median:", result_median)


# add vertical lines for mean and median
plt.axvline(result_mean, color="red", linestyle="--", label="Mean")
plt.axvline(result_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

"""**Box plots for identifying outliers in the numerical columns**"""

# box plot
sns.boxplot(x=df["age"])
plt.title("Box Plot for Age")
plt.xlabel("Age")
plt.show()

# box plot
sns.boxplot(x=df["result"])
plt.title("Box Plot for result")
plt.xlabel("result")
plt.show()

# count the outliers using IQR method
Q1 = df["age"].quantile(0.25)
Q3 = df["age"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
age_outliers = df[(df["age"] < lower_bound) | (df["age"] > upper_bound)]

len(age_outliers)

# count the outliers using IQR method
Q1 = df["result"].quantile(0.25)
Q3 = df["result"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
result_outliers = df[(df["result"] < lower_bound) | (df["result"] > upper_bound)]

len(result_outliers)

"""Univariate analysis of Categorical columns"""

categorical_columns = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',
       'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'gender',
       'ethnicity', 'jaundice', 'austim', 'contry_of_res', 'used_app_before',
       'relation']

for col in categorical_columns:
  sns.countplot(x=df[col])
  plt.title(f"Count Plot for {col}")
  plt.xlabel(col)
  plt.ylabel("Count")
  plt.show()

# countplot for target column (Class/ASD)
sns.countplot(x=df["Class/ASD"])
plt.title("Count Plot for Class/ASD")
plt.xlabel("Class/ASD")
plt.ylabel("Count")
plt.show()

"""handle missing values in ethnicity and relation column"""

df["ethnicity"] = df["ethnicity"].replace({"?": "Others", "others": "Others"})
df["relation"] = df["relation"].replace(
    {"?": "Others",
     "Relative": "Others",
     "Parent": "Others",
     "Health care professional": "Others"}
)
test_df["ethnicity"]=test_df["ethnicity"].replace({"?": "Others", "others": "Others"})
test_df["relation"]=test_df["relation"].replace({"?": "Others",
     "Relative": "Others",
     "Parent": "Others",
     "Health care professional": "Others"})

df["relation"].unique()

test_df["relation"].unique()

"""**Label Encoding**"""

# identify columns with "object" data type
object_columns = df.select_dtypes(include=["object"]).columns
object_test_columns=test_df.select_dtypes(include=["object"]).columns

print(object_columns)
print(object_test_columns)

# initialize a dictionary to store the encoders
encoders = {}

# apply label encoding and store the encoders
for column in object_columns:
  label_encoder = LabelEncoder()
  df[column] = label_encoder.fit_transform(df[column])
  encoders[column] = label_encoder   # saving the encoder for this column

encoders_test={}
for column in object_test_columns:
  label_encoder = LabelEncoder()
  test_df[column] = label_encoder.fit_transform(test_df[column])
  encoders_test[column] = label_encoder   # saving the encoder for this column


# save the encoders as a pickle file
with open("encoders.pkl", "wb") as f:
  pickle.dump(encoders, f)
encoders

"""# Bivariate Analysis"""

# correlation matrix
plt.figure(figsize=(15, 15))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation heatmap")
plt.show()

"""Insights from EDA:

1. few outliers in the numerical columns (age, results)

2. class imbalance in the target column

3. class imbalance in the categorical features

4. don't have any highly correlated column

5. performed label encoding and saved the encoders
"""

sns.countplot(y = 'ethnicity',hue='Class/ASD', data = df)
plt.xlabel('Qantity')
plt.ylabel('ethnicity')
plt.title('Autism for ethnicity?')

#Figure size
plt.figure(figsize=(10,4))

# Histogram
sns.histplot(data=df, x='age', hue='Class/ASD', binwidth=1, kde=True)

# Aesthetics
plt.title('Age distribution')
plt.xlabel('Age (years)')

"""Data preprocessing

Handling  outliers
"""

# function to replace the outliers with median
def replace_outliers_with_median(df, column):
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1

  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR

  median = df[column].median()

  # replace outliers with median value
  df[column] = df[column].apply(lambda x: median if x < lower_bound or x > upper_bound else x)

  return df

# replace outliers in the "age" column
df = replace_outliers_with_median(df, "age")

# replace outliers in the "result" column
df = replace_outliers_with_median(df, "result")

test_df=replace_outliers_with_median(test_df,"age")
test_df=replace_outliers_with_median(test_df,"result")

df.shape

"""**Train Test Split**"""

X = df.drop(columns=["Class/ASD"])
y = df["Class/ASD"]
X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape

y_train.value_counts()

"""# SMOTE (Synthetic Minority Oversampling technique)"""

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
print(y_train_smote.shape)

print(y_train_smote.value_counts())

"""# Model Training"""

# dictionary of classifiers
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42),
    "Ridge Classifier" : RidgeClassifier(random_state=42)
}
# dictionary to store the cross validation results
cv_scores = {}

# perform 5-fold cross validation for each model
for model_name, model in models.items():
  print(f"Training {model_name} with default parameters...")
  scores = cross_val_score(model, X_train_smote, y_train_smote, cv=10, scoring="accuracy")
  cv_scores[model_name] = scores
  print(f"{model_name} Cross-Validation Accuracy: {np.mean(scores):.2f}")
  print("-"*50)

cv_scores

"""# Model Selection & Hyperparameter Tuning"""

# Initializing models
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42)
xgboost_classifier = XGBClassifier(random_state=42)
logistic_regression=LogisticRegression(random_state=42)
Ridge = RidgeClassifier(random_state=42)

# Hyperparameter grids for RandomizedSearchCV

param_grid_dt = {
    "criterion": ["gini", "entropy"],
    "max_depth": [None, 10, 20, 30, 50, 60,70],
    "min_samples_split": [2, 5,7, 10],
    "min_samples_leaf": [1, 2,3, 4]
}


param_grid_rf = {
    "n_estimators": [50, 100, 200, 400,500],
    "max_depth": [None, 10, 20, 30,40],
    "min_samples_split": [2, 5, 7,10],
    "min_samples_leaf": [1, 2,3, 4],
    "bootstrap": [True, False]
}


param_grid_xgb = {
    "n_estimators": [50, 100, 200, 300,500],
    "max_depth": [3, 5, 7,9, 10],
    "learning_rate": [0.01, 0.1, 0.2, 0.3,0.4],
    "subsample": [0.5, 0.7, 0.8,1.0],
    "colsample_bytree": [0.5, 0.7,0.9, 1.0]
}
param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'max_iter': [100, 200, 500, 1000]
}
param_grid_ridge = {
    'alpha': [0.1, 1.0, 10.0, 100.0],
    'fit_intercept': [True, False],
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'],
    'max_iter': [None, 1000, 2000, 3000]
}

# hyperparameter tunig for 3 tree based models

# the below steps can be automated by using a for loop or by using a pipeline

# perform RandomizedSearchCV for each model
random_search_dt = RandomizedSearchCV(estimator=decision_tree, param_distributions=param_grid_dt, n_iter=20, cv=5, scoring="accuracy", random_state=42)
random_search_rf = RandomizedSearchCV(estimator=random_forest, param_distributions=param_grid_rf, n_iter=20, cv=5, scoring="accuracy", random_state=42)
random_search_xgb = RandomizedSearchCV(estimator=xgboost_classifier, param_distributions=param_grid_xgb, n_iter=20, cv=5, scoring="accuracy", random_state=42)
random_search_rc=RandomizedSearchCV(estimator=Ridge,param_distributions=param_grid_ridge,n_iter=20,cv=5,scoring="accuracy",random_state=42)
random_search_lr=RandomizedSearchCV(estimator=logistic_regression,param_distributions=param_grid_lr,n_iter=20,cv=5,scoring="accuracy",random_state=42)

# fit the models
random_search_dt.fit(X_train_smote, y_train_smote)
random_search_rf.fit(X_train_smote, y_train_smote)
random_search_xgb.fit(X_train_smote, y_train_smote)
random_search_lr.fit(X_train_smote, y_train_smote)
random_search_rc.fit(X_train_smote, y_train_smote)

# Get the model with best score

best_model = None
best_score = 0

if random_search_dt.best_score_ > best_score:
  best_model = random_search_dt.best_estimator_
  best_score = random_search_dt.best_score_

if random_search_rf.best_score_ > best_score:
  best_model = random_search_rf.best_estimator_
  best_score = random_search_rf.best_score_

if random_search_xgb.best_score_ > best_score:
  best_model = random_search_xgb.best_estimator_
  best_score = random_search_xgb.best_score_

if random_search_lr.best_score_ > best_score:
  best_model = random_search_lr.best_estimator_
  best_score = random_search_lr.best_score_

if random_search_rc.best_score_ > best_score:
  best_model = random_search_rc.best_estimator_
  best_score = random_search_rc.best_score_

random_search_rc.best_score_

print(f"Best Model: {best_model}")
print(f"Best Cross-Validation Accuracy: {best_score:.2f}")

# save the best model
with open("best_model.pkl", "wb") as f:
  pickle.dump(best_model, f)

"""# Evaluation"""

# evaluate on test data
y_test_pred = best_model.predict(X_test)
print("Accuracy score:\n", accuracy_score(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
print("Classification Report:\n", classification_report(y_test, y_test_pred))

ext = ExtraTreesClassifier(max_depth=5,min_samples_split=2 ,min_samples_leaf=5, random_state= 42, criterion='entropy')
ext.fit(X_train, y_train)
ext.score(X_train, y_train)

import pandas as pd
x_test = np.array(test_df.drop('ID',axis=1))
y_ans=best_model.predict(x_test)
y_ans = pd.Series(y_ans)
y_ans.head()
submission = pd.DataFrame({'ID':test_df['ID'],
                           'Class/ASD':y_ans})
submission.head()

submission.to_csv("submission.csv",index=False)

"""# EXTRA"""

from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train, X_test, y_train, y_test)

models

# Sort the models by accuracy in descending order (highest first)
top_models = models.sort_values(by='Accuracy', ascending=False).head(5)

# Plot the top 5 models and their accuracies
plt.figure(figsize=(10, 6))
bars = plt.barh(top_models.index, top_models['Accuracy'], color='skyblue')
plt.xlabel('Accuracy')
plt.title('Top 5 Models with Highest Accuracy')

# Display percentages on top of the bars
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2%}',
             va='center', color='black', fontsize=10)

plt.show()